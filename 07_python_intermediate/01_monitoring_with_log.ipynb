{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Python Logging**\n",
    "\n",
    "The logging module in Python provides a flexible framework for emitting log messages from your code. Logs are essential for understanding and debugging your program, especially in production environments or when you're working with complex systems like web scraping.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why Use Logging?**\n",
    "1. **Debugging:** Helps in tracking program execution without cluttering the code with `print()` statements.\n",
    "2. **Persistence:** Logs can be saved to a file, enabling analysis after the program finishes.\n",
    "3. **Control:** You can set logging levels to filter messages based on their importance.\n",
    "4. **Structured Output:** With proper configuration, logs can include timestamps, severity levels, and more.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Basic Concepts in Logging**\n",
    "1. **Loggers:** The main entry point for logging. You can think of them as entities that emit log messages.\n",
    "2. **Handlers:** Define where the log messages go (console, file, etc.).\n",
    "3. **Levels:** Determine the severity of a log message. Common levels are:\n",
    "   - `DEBUG`: Detailed information for diagnosing problems.\n",
    "   - `INFO`: Confirmation that things are working as expected.\n",
    "   - `WARNING`: An indication of something unexpected or an issue that isn’t critical yet.\n",
    "   - `ERROR`: A serious problem that prevents the program from continuing.\n",
    "   - `CRITICAL`: A very serious error, often indicating a program crash.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Basic Logging Example**\n",
    "\n",
    "Here’s how to get started with Python's logging module:\n",
    "\n",
    "```python\n",
    "import logging\n",
    "\n",
    "# Set up a basic logger\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the minimum logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Define the log message format\n",
    ")\n",
    "\n",
    "# Example log messages\n",
    "logging.debug(\"This is a debug message. Used for detailed diagnostic output.\")\n",
    "logging.info(\"This is an info message. Indicates the program is running as expected.\")\n",
    "logging.warning(\"This is a warning message. Something unexpected happened.\")\n",
    "logging.error(\"This is an error message. A problem occurred.\")\n",
    "logging.critical(\"This is a critical message. A serious error happened.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Output Explanation**\n",
    "When you run the code, you'll see output like this:\n",
    "\n",
    "```\n",
    "2024-12-23 14:23:01,123 - DEBUG - This is a debug message. Used for detailed diagnostic output.\n",
    "2024-12-23 14:23:01,124 - INFO - This is an info message. Indicates the program is running as expected.\n",
    "2024-12-23 14:23:01,125 - WARNING - This is a warning message. Something unexpected happened.\n",
    "2024-12-23 14:23:01,126 - ERROR - This is an error message. A problem occurred.\n",
    "2024-12-23 14:23:01,127 - CRITICAL - This is a critical message. A serious error happened.\n",
    "```\n",
    "\n",
    "- **Timestamp:** Indicates when the log was recorded.\n",
    "- **Log Level:** Shows the severity of the log message.\n",
    "- **Message:** The custom message provided.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Functions**\n",
    "1. **`logging.basicConfig()`**: Sets up the configuration for logging.\n",
    "2. **Logging methods:** These emit messages with a severity level:\n",
    "   - `logging.debug()`\n",
    "   - `logging.info()`\n",
    "   - `logging.warning()`\n",
    "   - `logging.error()`\n",
    "   - `logging.critical()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping completed successfully.\n",
      "Scraping page 2\n",
      "Scraping completed successfully.\n",
      "Scraping page 3\n",
      "Scraping completed successfully.\n",
      "Scraping page 4\n",
      "Scraping completed successfully.\n",
      "Scraping page 5\n",
      "Scraping completed successfully.\n",
      "Scraping page 6\n",
      "Scraping completed successfully.\n",
      "Scraping page 7\n",
      "Scraping completed successfully.\n",
      "Scraping page 8\n",
      "Scraping completed successfully.\n",
      "Scraping page 9\n",
      "Scraping completed successfully.\n",
      "Scraping page 10\n",
      "Scraping completed successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Example of scraping process\n",
    "\"\"\"\n",
    "def start_scraping():\n",
    "    # Scraping page 1 to 10\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Scraping page {i}\")\n",
    "        time.sleep(1)\n",
    "        # Getting page response\n",
    "        print(\"Scraping completed successfully.\")\n",
    "\n",
    "start_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scraping page 1\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 2\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 3\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 4\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 5\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 6\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 7\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 8\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 9\n",
      "INFO:root:Scraping completed successfully.\n",
      "INFO:root:Scraping page 10\n",
      "INFO:root:Scraping completed successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Understand the basics of Python's logging module and why it's important. \n",
    "Logging helps you monitor your program's behavior and debug issues without relying on print statements.\n",
    "\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# 1. Set up a basic logger that logs messages at the INFO level.\n",
    "# 2. Replace the start_scraping print statement with logging message.\n",
    "# 3. Log the following messages with info level:\n",
    "#    - \"Scraping page \" following by the page number\n",
    "#    - \"Scraping completed successfully.\"\n",
    "import logging\n",
    "import time \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "def start_scraping():\n",
    "    # Scraping page 1 to 10\n",
    "    for i in range(1, 11):\n",
    "        logging.info(f\"Scraping page {i}\")\n",
    "        time.sleep(1)\n",
    "        # Getting page response\n",
    "        logging.info(\"Scraping completed successfully.\")\n",
    "\n",
    "start_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:invalid response\n",
      "ERROR:root:invalid response\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Setup different logs level\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Setup a logger that only log error messages\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, force=True)\n",
    "\n",
    "def scraping_with_error_response():\n",
    "    response_code = [200, 200, 200, 200, 200, 404, 503]\n",
    "\n",
    "    # Scraping page 1 to 10\n",
    "    for i in range(1,11):\n",
    "        # TODO: Add log message for tracking page number\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Getting page response\n",
    "        response = random.choice(response_code)\n",
    "        \n",
    "        if response == 200:   \n",
    "            # TODO: Add log message for valid response\n",
    "            logging.info(\"valid response\")\n",
    "\n",
    "        else:\n",
    "            # TODO: Add log message for invalid response\n",
    "            logging.error(\"invalid response\")\n",
    "\n",
    "\n",
    "scraping_with_error_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective: Learn to configure logging to log messages to a file for persistent records. \n",
    "This is useful for analyzing scraping sessions or debugging after the program runs.\n",
    "\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# 1. Configure logging to log messages at the DEBUG level to a file named `scraper.log`.\n",
    "# 2. Add timestamps to the log messages.\n",
    "# 3. Use previous function for this task\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    filename=\"scraper.log\",\n",
    "    force=True, \n",
    "    format=\"[%(asctime)s] %(levelname)s \\t- %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def scraping_with_error_response():\n",
    "    response_code = [200, 200, 200, 200, 200, 404, 503]\n",
    "\n",
    "    # Scraping page 1 to 10\n",
    "    for i in range(1,11):\n",
    "        # TODO: Add log message for tracking page number\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Getting page response\n",
    "        response = random.choice(response_code)\n",
    "        \n",
    "        if response == 200:   \n",
    "            # TODO: Add log message for valid response\n",
    "            logging.info(f\"valid response, response code: {response}\")\n",
    "\n",
    "        else:\n",
    "            # TODO: Add log message for invalid response\n",
    "            logging.error(f\"invalid response, response code: {response}\")\n",
    "\n",
    "\n",
    "scraping_with_error_response()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective: Apply logging to a full scraping workflow and use different logging levels for various stages.\n",
    "This will help you monitor and troubleshoot scraping operations more effectively.\n",
    "\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# 1. Write a script that:\n",
    "#    - Logs INFO when scraping starts.\n",
    "#    - Logs DEBUG for each URL being processed.\n",
    "#    - Logs ERROR if a request fails.\n",
    "#    - Logs INFO when scraping ends.\n",
    "# 2. Scrape data from multiple URLs, including one invalid URL to test the error logging.\n",
    "\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, \n",
    "    force=True, \n",
    "    filename=\"scrape_link.log\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    format=\"[%(asctime)s] %(levelname)s \\t- %(message)s\"\n",
    ")\n",
    "\n",
    "def scrape_links(links):\n",
    "    logging.info(\"Scraping started.\")\n",
    "    \n",
    "    for link in links:\n",
    "        logging.debug(f\"Processing link: {link}\")\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            response.raise_for_status()\n",
    "            logging.info(f\"Successfully scraped link: {link}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Failed to scrape link: {link} - {e}\")\n",
    "\n",
    "    logging.info(\"Scraping completed.\")\n",
    "\n",
    "links = [\n",
    "    \"https://www.google.com\",  # Valid\n",
    "    \"https://www.github.com\",  # Valid\n",
    "    \"https://www.python.org\",  # Valid\n",
    "    \"https://www.openai.com\",  # Valid\n",
    "    \"https://example.com\",  # Valid\n",
    "    \"htp://invalid-url.com\",  # Invalid (typo in \"http\")\n",
    "    \"https://invalid domain.com\",  # Invalid (space in domain)\n",
    "    \"ftp://files.example.com\",  # Valid but not an HTTP link\n",
    "    \"http://256.256.256.256\",  # Invalid (IP out of range)\n",
    "    \"https://thiswebsitedoesnotexist.xyz\"  # Likely Invalid (random domain)\n",
    "]\n",
    "\n",
    "scrape_links(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective: Explore advanced logging by using custom handlers to log messages to multiple destinations. \n",
    "This technique improves flexibility in handling log output.\n",
    "\"\"\"\n",
    "# Create handlers\n",
    "console_handler = logging.StreamHandler() # This will shows log message in the console\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# TODO: \n",
    "# 1. Create another handler for storing log in a file using logging.FileHandler('error.log')\n",
    "# 2. Set the level to DEBUG\n",
    "file_handler = logging.FileHandler('error.log')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Attach formatter to handlers\n",
    "console_handler.setFormatter(formatter)\n",
    "# TODO: Add formatter to the file handler\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "# Create logger and attach handlers\n",
    "logger = logging.getLogger('ScraperLogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(console_handler) # Attach stream handler into the logger object\n",
    "# TODO: Attach the file handler into the logger object\n",
    "logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,952 - INFO - Scraping started.\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:46,967 - DEBUG - Processing link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,545 - INFO - Successfully scraped link: https://www.google.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:47,555 - DEBUG - Processing link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,295 - INFO - Successfully scraped link: https://www.github.com\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:49,308 - DEBUG - Processing link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,888 - INFO - Successfully scraped link: https://www.python.org\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:50,898 - DEBUG - Processing link: https://www.openai.com\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,071 - ERROR - Failed to scrape link: https://www.openai.com - 403 Client Error: Forbidden for url: https://www.openai.com/\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,080 - DEBUG - Processing link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,726 - INFO - Successfully scraped link: https://example.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,734 - DEBUG - Processing link: htp://invalid-url.com\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,742 - ERROR - Failed to scrape link: htp://invalid-url.com - No connection adapters were found for 'htp://invalid-url.com'\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,747 - DEBUG - Processing link: https://invalid domain.com\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,755 - ERROR - Failed to scrape link: https://invalid domain.com - HTTPSConnectionPool(host='invalid%20domain.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc95568a0>: Failed to resolve 'invalid%20domain.com' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,767 - DEBUG - Processing link: ftp://files.example.com\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,780 - ERROR - Failed to scrape link: ftp://files.example.com - No connection adapters were found for 'ftp://files.example.com'\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,789 - DEBUG - Processing link: http://256.256.256.256\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,827 - ERROR - Failed to scrape link: http://256.256.256.256 - HTTPConnectionPool(host='256.256.256.256', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x730dc9551520>: Failed to resolve '256.256.256.256' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,834 - DEBUG - Processing link: https://thiswebsitedoesnotexist.xyz\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,879 - ERROR - Failed to scrape link: https://thiswebsitedoesnotexist.xyz - HTTPSConnectionPool(host='thiswebsitedoesnotexist.xyz', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x730dc954f230>: Failed to resolve 'thiswebsitedoesnotexist.xyz' ([Errno -2] Name or service not known)\"))\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n",
      "2025-03-15 13:15:51,885 - INFO - Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Handling failed requests using logging\n",
    "\"\"\"\n",
    "# TODO:\n",
    "# 1. Create a function that loop through number and get the random response,\n",
    "# just like previous code but modify it as you like\n",
    "# 2. Handle stream log in the console and the error log in a file\n",
    "# 3. Provide a file that contains all of failed URL so you can retry again\n",
    "# 4. Automate the process (optional)\n",
    "\n",
    "def scrape_links(links):\n",
    "    logger.info(\"Scraping started.\")\n",
    "    \n",
    "    for link in links:\n",
    "        logger.debug(f\"Processing link: {link}\")\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Successfully scraped link: {link}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Failed to scrape link: {link} - {e}\")\n",
    "\n",
    "    logger.info(\"Scraping completed.\")\n",
    "\n",
    "links = [\n",
    "    \"https://www.google.com\",  # Valid\n",
    "    \"https://www.github.com\",  # Valid\n",
    "    \"https://www.python.org\",  # Valid\n",
    "    \"https://www.openai.com\",  # Valid\n",
    "    \"https://example.com\",  # Valid\n",
    "    \"htp://invalid-url.com\",  # Invalid (typo in \"http\")\n",
    "    \"https://invalid domain.com\",  # Invalid (space in domain)\n",
    "    \"ftp://files.example.com\",  # Valid but not an HTTP link\n",
    "    \"http://256.256.256.256\",  # Invalid (IP out of range)\n",
    "    \"https://thiswebsitedoesnotexist.xyz\"  # Likely Invalid (random domain)\n",
    "]\n",
    "\n",
    "scrape_links(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "In what situation logging will help you a lot?\n",
    "\n",
    "to debugging, troubleshooting and monitoring app performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration**\n",
    "Explore advanced log and monitoring tools like:\n",
    "- Loguru\n",
    "- Loggly\n",
    "- Datadog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
